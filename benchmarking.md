# ğŸ§ª What Is Benchmarking?

**Benchmarking** is the process of **measuring the performance** of your code.

Not â€œdoes it work?â€

But:

* How fast does it run?
* How much memory does it use?
* How does it compare to another implementation?

---

# ğŸ§  In Simple Terms

Testing checks correctness:

> âœ… â€œDoes Vec2::length() return 5 for (3,4)?â€

Benchmarking checks performance:

> âš¡ â€œHow fast can we compute length() 10 million times?â€

---

# ğŸ§® Example (Your Vec2)

Suppose you want to test how fast normalization is.

You could write:

```cpp
#include <iostream>
#include <chrono>
#include "vec2.hpp"

using namespace vec2lib;

int main() {
    Vec2 v(3.0, 4.0);

    const int iterations = 10'000'000;

    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < iterations; ++i) {
        v.normalized();
    }

    auto end = std::chrono::high_resolution_clock::now();

    std::chrono::duration<double> elapsed = end - start;

    std::cout << "Time taken: " << elapsed.count() << " seconds\n";
}
```

That is a **benchmark**.

---

# ğŸ“Š Why Benchmarking Matters

For math libraries especially:

* Linear algebra must be fast
* Game engines require real-time math
* Robotics systems need predictable timing
* Physics simulations run millions of operations

Performance matters.

---

# ğŸ”¬ What You Can Benchmark

For your Vec2:

* Dot product speed
* Normalization speed
* Operator overload speed
* `-O0` vs `-O3` comparison
* Double vs float performance
* With and without `inline`

---

# âš¡ Example: Debug vs Release

Run:

```bash
make DEBUG=1
make run
```

Then:

```bash
make
make run
```

Youâ€™ll likely see:

* Debug build â†’ slower
* Release build â†’ much faster

Thatâ€™s benchmarking in action.

---

# ğŸ§  Important Concept: Microbenchmark vs Real Benchmark

| Type             | Meaning                            |
| ---------------- | ---------------------------------- |
| Microbenchmark   | Measure tiny function (like dot()) |
| System Benchmark | Measure full system performance    |

You are doing **microbenchmarking** for now.

---

# ğŸš¨ Common Beginner Mistake

Compiler optimization may remove your code entirely.

Example:

```cpp
v.normalized();
```

If result is unused, compiler might delete it.

Fix:

```cpp
volatile Vec2 result;
result = v.normalized();
```

or accumulate result.

---

# ğŸ Why This Is Important For You

Since you're interested in:

* Systems programming
* Robotics
* Performance-oriented code

Benchmarking is a **core engineering skill**.

Serious engineers donâ€™t guess performance.

They measure it.

---

# ğŸ”§ Professional Tools (Later Stage)

When you go deeper:

* `std::chrono`
* `perf` (Linux)
* `valgrind --tool=callgrind`
* Google Benchmark library
* VTune (Intel)

But start simple.

---

# ğŸ¯ In One Sentence

Benchmarking is:

> Measuring how fast (or efficient) your code runs under controlled conditions.

---

Good â€” now we move from â€œcode runsâ€ â†’ â€œcode is measurableâ€.

To support **benchmarking**, we need to:

1. Add a `benchmarks/` folder
2. Compile benchmark files separately
3. Add a `benchmark` target
4. (Optional) Add special optimization flags

Iâ€™ll show you the clean upgrade to your current Makefile.

---

# ğŸ“ First: Add This Folder

Create:

```
benchmarks/
â””â”€â”€ bench_vec2.cpp
```

Example benchmark file:

```cpp
#include <iostream>
#include <chrono>
#include "vec2.hpp"

using namespace vec2lib;

int main() {
    Vec2 v(3.0, 4.0);
    const int iterations = 50'000'000;

    auto start = std::chrono::high_resolution_clock::now();

    double sum = 0.0;
    for (int i = 0; i < iterations; ++i) {
        sum += v.length();
    }

    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> elapsed = end - start;

    std::cout << "Time: " << elapsed.count() << " seconds\n";
    std::cout << "Dummy sum: " << sum << "\n";

    return 0;
}
```

The `sum` prevents optimization removal.

---

# ğŸ”§ Now Modify Your Makefile

Below are **only the additions and changes**.

---

## 1ï¸âƒ£ Add Benchmark Variables

Add near the top:

```make
BENCH_DIR := benchmarks
BENCH_SRCS := $(wildcard $(BENCH_DIR)/*.cpp)
BENCH_BINS := $(BENCH_SRCS:$(BENCH_DIR)/%.cpp=$(BIN_DIR)/%)
```

This auto-detects all benchmark files.

---

## 2ï¸âƒ£ Add Rule to Build Benchmark Executables

Add this rule:

```make
$(BIN_DIR)/%: $(BENCH_DIR)/%.cpp $(OBJ)
	@mkdir -p $(BIN_DIR)
	$(CXX) $(CXXFLAGS) $^ -o $@
```

This is similar to your test rule.

---

## 3ï¸âƒ£ Add Benchmark Target

Add:

```make
.PHONY: benchmark
benchmark: $(BENCH_BINS)
	@echo "Running benchmarks..."
	@for b in $(BENCH_BINS); do $$b; done
```

---

# âš¡ Optional (Recommended for Benchmarking)

Benchmarking should always use maximum optimization.

Add this special target:

```make
.PHONY: bench-opt
bench-opt:
	$(MAKE) CXXFLAGS="-std=c++20 -Wall -Wextra -Iinclude -O3 -march=native" benchmark
```

Now you can run:

```bash
make bench-opt
```

---

# ğŸ§  Why These Changes Matter

You now have:

* Automatic benchmark discovery
* Separate benchmark binaries
* Clean separation from tests
* Optimization control
* Scalable structure

---

# ğŸ How To Use

### Normal build

```bash
make
```

### Run tests

```bash
make run
```

### Run benchmarks

```bash
make benchmark
```

### Optimized benchmarking

```bash
make bench-opt
```

---

# ğŸ§  What You Just Learned

You expanded your build system to:

* Handle multiple executable categories
* Automatically detect files
* Control compiler flags per target
* Separate correctness testing from performance testing

Thatâ€™s real build-system maturity.

---